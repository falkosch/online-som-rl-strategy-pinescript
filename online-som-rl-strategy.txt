//@version=6
strategy("Online SOM-RL Strategy", overlay=true)

// === SOM Node definition
type SOMNode
    float[] state   // M-length input vector
    float[] q_values // expected returns by action (in same order as action_labels)

// === Configurable Inputs
source = input.source(close, title="Price source")

M = input.int(50, minval=20, maxval=150, step=10, title="Past price ticks for input vector")
P = input.int(20, minval=10, maxval=100, step=10, title="Future ticks to evaluate reward")
N = input.int(20, minval=10, maxval=50, step=5, title="Number of nodes in Q-table")

initial_exploration = input.float(0.1, minval=0.01, maxval=0.5, step=0.01, title="Initial exploration probability")
initial_sigma_factor = input.float(0.05, minval=0.01, maxval=0.2, step=0.01, title="Initial neighborhood width")
initial_beta = input.float(0.6, minval=0.1, maxval=1.0, step=0.05, title="Learning rate (beta)")
initial_gamma = input.float(0.9, minval=0.5, maxval=0.99, step=0.01, title="Discount factor (gamma)")
trading_penalty = input.float(0.02, minval=0.0, maxval=0.1, step=0.01, title="Trading penalty")

t_decay_factor = input.float(0.9992, minval=0.001, maxval=1.0, title="Time decay factor for learning parameters")
update_som_each_n_bar = input.int(2, minval=1, title="How many bars to skip updating the SOM")
delay_phase = input.int(150, minval=0, title="How many bars to wait before the SOM should start to learn")
warmup_phase = input.int(7000, minval=0, title="How many bars to let the SOM learn first")
numerical_epsilon = input.float(1e-6, minval=1e-8, maxval=1e-2, title="To avoid division by zero and improve numerical stability")

// === Cached metrics
source_p = source[P]
learning_start_index = math.max(M + P, delay_phase)
mean_price_mp = ta.sma(source_p, M)
std_price_mp = ta.stdev(source_p, M) + numerical_epsilon
std_price_p = ta.stdev(source, P) + numerical_epsilon
risk_adjusted_return = ta.sma((source - source_p) / std_price_p, P)
plot(risk_adjusted_return, color=color.new(color.green, 70), title="Risk-adjusted Return")

// === dynamic learning parameters
t_decay = math.pow(t_decay_factor, bar_index - learning_start_index) + numerical_epsilon
exploration = initial_exploration * t_decay
sigma = N * initial_sigma_factor * t_decay
beta = initial_beta * t_decay
gamma = initial_gamma * t_decay
plot(t_decay, color=color.new(color.orange, 80), title="t_decay")

// === Action options
HOLD_ACTION_LABEL = "HOLD"
action_labels = array.from(HOLD_ACTION_LABEL, "INV_500", "INV_1000", "INV_2000", "INV_5000", "SELL_500", "SELL_1000", "SELL_2000", "SELL_5000")
num_actions = array.size(action_labels)

// === Random initialize vector
new_random_vector(int size, float base = 0.0, float max = 1.0, float min = 0.0) =>
    v = array.new_float(size)
    last = size - 1
    for j = 0 to last
        array.set(v, j, base + math.random(min, max))
    v

// === Get current input vector of M points with an offset to current bar_index
get_input_vector(int bars, int offset = 0) =>
    v = array.new_float(bars * 2)
    last = bars - 1
    for i = 0 to last
        j = offset + i
        // Z-score normalize price
        price_raw = source[j]
        price_val = na(price_raw) ? 0.0 : price_raw
        price_z = na(mean_price_mp) ? price_val : ((price_val - mean_price_mp) / std_price_mp)
        // array.set(v, i, price_val)
        // Log normalize volume
        volume_raw = volume[j]
        volume_val = na(volume_raw) ? 0.0 : math.log(volume_raw + 1.0)
        array.set(v, i * 2, price_val)
        array.set(v, i * 2 + 1, volume_val)
    v

// === Computes tangent hyperbolic
tanh(x) =>
    exp_x_pos = math.exp(x)
    exp_x_neg = math.exp(-x)
    (exp_x_pos - exp_x_neg) / (exp_x_pos + exp_x_neg + numerical_epsilon)

// === Compute squared distance
squared_distance(float[] a, float[] b) =>
    last = a.size() - 1
    d = 0.0
    for i = 0 to last
        diff = array.get(a, i) - array.get(b, i)
        d += diff * diff
    d

cosine_distance(float[] a, float[] b) =>
    last = a.size() - 1
    norm_a = 0.0
    norm_b = 0.0
    dot_ab = 0.0
    for i = 0 to last
        value_a = array.get(a, i)
        value_b = array.get(b, i)
        norm_a += value_a * value_a
        norm_b += value_b * value_b
        dot_ab += value_a * value_b
    1.0 - dot_ab / (math.sqrt(norm_a * norm_b) + numerical_epsilon)

distance(float[] a, float[] b) =>
    cosine_distance(a, b)
    // squared_distance(a, b)

// === SOM Initialization
var som = array.new<SOMNode>(N)
if bar_index == 0
    state_size = get_input_vector(M).size()
    som_last = N - 1
    for i = 0 to som_last
        array.set(som, i, SOMNode.new(new_random_vector(state_size), new_random_vector(num_actions, 0.001, 0.0)))

// === Finds state vector matching best with input vector x
find_best_node(float[] x) =>
    best_i = 0
    best_node = array.get(som, 0)
    best_d = distance(x, best_node.state)
    last = N - 1
    for i = 1 to last
        node = array.get(som, i)
        d = distance(x, node.state)
        if d < best_d
            best_i := i
            best_node := node
            best_d := d
    [best_i, best_d, best_node]

// === Finds best q-value in SOMNode
find_best_q(SOMNode node, p_exploration = 0.0) =>
    if math.random(0.0, 1.0) <= p_exploration
        explore_i = math.floor(math.random(0, num_actions))
        [explore_i, node.q_values.get(explore_i)]
    else
        best_i = 0
        best_q = array.get(node.q_values, 0)
        last = num_actions - 1
        for i = 1 to last
            q = array.get(node.q_values, i)
            if q > best_q
                best_i := i
                best_q := q
        [best_i, best_q]

// === Computes Gaussian neighborhood kernel
sigma_squared_2 = 2.0 * sigma * sigma + numerical_epsilon
guassian_kernel(int winner_i, int node_i) =>
    d = winner_i - node_i
    math.exp(-d * d / sigma_squared_2)

// === Updates neighbourhood states using 1D Gaussian kernel
update_som_with_neighbors(int winner_i, float[] input) =>
    node_last = N - 1
    for node_i = 0 to node_last
        node = array.get(som, node_i)
        h = guassian_kernel(winner_i, node_i)
        state = node.state
        state_last = state.size() - 1
        for i = 0 to state_last
            input_value = array.get(input, i)
            old = array.get(state, i)
            array.set(state, i, old + beta * h * (input_value - old))

// Rewards
reward(string action_label, float current_position) =>
    // Base return
    base_return = (source - source_p) / std_price_p
    // Volatility penalty for high-risk periods
    volatility_penalty = math.min(std_price_p * 0.1, 0.05)
    // Position sizing penalty (discourage overtrading)
    position_penalty = math.abs(current_position) * 0.001
    // Directional consistency bonus
    price_direction = math.sign(source - source[1])
    action_direction = str.contains(action_label, "INV") ? 1.0 : 
                      str.contains(action_label, "SELL") ? -1.0 : 0.0
    directional_bonus = price_direction == action_direction ? 0.01 : 0.0
    // Trading cost
    trading_cost = action_label == "HOLD" ? 0.0 : trading_penalty
    // Squash values to compensate excessive rewards
    tanh(base_return - volatility_penalty - position_penalty - trading_cost + directional_bonus)

if (bar_index >= learning_start_index) and (bar_index < last_bar_index)
    // First perform Online Learning (per bar)
    // Get current state
    input_now = get_input_vector(M, P)
    input_next = get_input_vector(M)
    [best_i_now, _, best_node_now] = find_best_node(input_now)
    [best_q_i_now, _] = find_best_q(best_node_now, exploration)
    action_label = array.get(action_labels, best_q_i_now)
    // Update SOM
    if 0 == (bar_index % update_som_each_n_bar)
        update_som_with_neighbors(best_i_now, input_now)
    // Get next state
    [_, _, next_node] = find_best_node(input_next)
    [_, next_best_q] = find_best_q(next_node)
    // Update q values
    q_values = best_node_now.q_values
    old_q = array.get(q_values, best_q_i_now)
    r = reward(action_label, strategy.opentrades.capital_held)
    array.set(q_values, best_q_i_now, old_q + beta * (r + gamma * next_best_q - old_q))

    // Then trade when warm up is done
    if (bar_index > learning_start_index + warmup_phase)
        // === Action Selection on Last Bar
        [best_i, best_d, best_node] = find_best_node(input_next)
        [best_q_i, best_q] = find_best_q(best_node)
        action_label = action_labels.get(best_q_i)
        log.info("bar {0}, {1} - i={2}, d={3}, q_i={4}, q={5}, a={6}", bar_index, t_decay, best_i, best_d, best_q_i, best_q, action_label)

        if str.startswith(action_label, "INV")
            strategy.entry("SOM", strategy.long, qty=str.tonumber(str.replace(action_label, "INV_", "")))
        else if str.startswith(action_label, "SELL")
            strategy.entry("SOM", strategy.short, qty=str.tonumber(str.replace(action_label, "SELL_", "")))
        // HOLD does nothing

plot(math.log(1+strategy.opentrades.capital_held), color=color.new(color.blue, 80), title="Log open trades capitel held")
