//@version=6
strategy("Online SOM-RL Strategy", overlay=true)

// === SOM Node definition
type SOMNode
    float[] state   // M-length input vector
    float[] q_values // expected returns by action (in same order as action_labels)

// === Configurable Inputs
source = input.source(close, title="Price source")

M = input.int(50, minval=20, maxval=150, step=10, title="Past price ticks for input vector")
P = input.int(20, minval=10, maxval=100, step=10, title="Future ticks to evaluate reward")
N = input.int(20, minval=10, maxval=50, step=5, title="Number of nodes in Q-table")

initial_exploration = input.float(0.1, minval=0.01, maxval=0.5, step=0.01, title="Initial exploration probability")
initial_sigma_factor = input.float(0.05, minval=0.01, maxval=0.2, step=0.01, title="Initial neighborhood width")
initial_beta = input.float(0.6, minval=0.1, maxval=1.0, step=0.05, title="Learning rate (beta)")
initial_gamma = input.float(0.9, minval=0.5, maxval=0.99, step=0.01, title="Discount factor (gamma)")
trading_penalty = input.float(0.02, minval=0.0, maxval=0.1, step=0.01, title="Trading penalty")

// === Reward function parameters
volatility_penalty_factor = input.float(0.1, minval=0.01, maxval=0.5, step=0.01, title="Volatility penalty factor")
volatility_penalty_cap = input.float(0.05, minval=0.01, maxval=0.2, step=0.01, title="Maximum volatility penalty")
position_penalty_factor = input.float(0.001, minval=0.0001, maxval=0.01, step=0.0001, title="Position sizing penalty factor")
directional_bonus = input.float(0.01, minval=0.0, maxval=0.05, step=0.001, title="Directional consistency bonus")

// === Algorithm parameters
distance_function = input.string("cosine", options=["cosine", "euclidean"], title="Distance function for SOM")

t_decay_factor = input.float(0.9992, minval=0.001, maxval=1.0, title="Time decay factor for learning parameters")
update_som_each_n_bar = input.int(2, minval=1, title="How many bars to skip updating the SOM")
delay_phase = input.int(150, minval=0, title="How many bars to wait before the SOM should start to learn")
warmup_phase = input.int(7000, minval=0, title="How many bars to let the SOM learn first")
numerical_epsilon = input.float(1e-6, minval=1e-8, maxval=1e-2, title="To avoid division by zero and improve numerical stability")

// === Cached metrics
source_p = source[P]
learning_start_index = math.max(M + P, delay_phase)
mean_price_mp = ta.sma(source_p, M)
std_price_mp = ta.stdev(source_p, M) + numerical_epsilon
std_price_p = ta.stdev(source, P) + numerical_epsilon
risk_adjusted_return = ta.sma((source - source_p) / std_price_p, P)
plot(risk_adjusted_return, color=color.new(color.green, 70), title="Risk-adjusted Return")

// === dynamic learning parameters
t_decay = math.pow(t_decay_factor, bar_index - learning_start_index) + numerical_epsilon
exploration = initial_exploration * t_decay
sigma = N * initial_sigma_factor * t_decay
beta = initial_beta * t_decay
gamma = initial_gamma * t_decay
plot(t_decay, color=color.new(color.orange, 80), title="t_decay")

// === Action options
HOLD_ACTION_LABEL = "HOLD"
action_labels = array.from(HOLD_ACTION_LABEL, "INV_500", "INV_1000", "INV_2000", "INV_5000", "SELL_500", "SELL_1000", "SELL_2000", "SELL_5000")
num_actions = array.size(action_labels)

// Creates a new array of specified size filled with random values within given range.
new_random_vector(int vector_size, float base_value = 0.0, float min_value = 0.0, float max_value = 1.0) =>
    if vector_size <= 0
        runtime.error("Vector size must be positive")
    if min_value > max_value
        runtime.error("Min value cannot be greater than max value")
    
    vector = array.new_float(vector_size)
    last_index = vector_size - 1
    for index = 0 to last_index
        random_value = base_value + math.random(min_value, max_value)
        array.set(vector, index, random_value)
    vector

// Constructs feature vector with normalized price and log-normalized volume for specified number of historical bars.
get_input_vector(int bars, int bar_offset = 0) =>
    if bars <= 0
        runtime.error("Number of bars must be positive")
    
    input_vector = array.new_float(bars * 2)
    last_bar_idx = bars - 1
    
    for bar_idx = 0 to last_bar_idx
        historical_offset = bar_offset + bar_idx
        
        // Get normalized price
        price_raw = source[historical_offset]
        price_normalized = na(price_raw) ? 0.0 : price_raw
        
        // Get log-normalized volume
        volume_raw = volume[historical_offset]
        volume_normalized = na(volume_raw) ? 0.0 : math.log(volume_raw + 1.0)
        
        // Store in vector (price, volume pairs)
        array.set(input_vector, bar_idx * 2, price_normalized)
        array.set(input_vector, bar_idx * 2 + 1, volume_normalized)
    
    input_vector

// Calculates hyperbolic tangent function for reward normalization.
tanh(x) =>
    exp_x_pos = math.exp(x)
    exp_x_neg = math.exp(-x)
    (exp_x_pos - exp_x_neg) / (exp_x_pos + exp_x_neg + numerical_epsilon)

// Calculates Euclidean distance squared between two vectors for SOM similarity measurement.
squared_distance(float[] a, float[] b) =>
    last = a.size() - 1
    d = 0.0
    for i = 0 to last
        diff = array.get(a, i) - array.get(b, i)
        d += diff * diff
    d

// Calculates cosine distance (1 - cosine similarity) between two vectors for SOM similarity measurement.
cosine_distance(float[] a, float[] b) =>
    last = a.size() - 1
    norm_a = 0.0
    norm_b = 0.0
    dot_ab = 0.0
    for i = 0 to last
        value_a = array.get(a, i)
        value_b = array.get(b, i)
        norm_a += value_a * value_a
        norm_b += value_b * value_b
        dot_ab += value_a * value_b
    1.0 - dot_ab / (math.sqrt(norm_a * norm_b) + numerical_epsilon)

// Selects and applies the configured distance function for vector similarity calculation.
distance(float[] a, float[] b) =>
    switch distance_function
        "cosine" => cosine_distance(a, b)
        "euclidean" => squared_distance(a, b)
        => cosine_distance(a, b)  // default fallback

// === SOM Initialization
var som = array.new<SOMNode>(N)
if bar_index == 0
    // Calculate state size without creating unnecessary arrays
    state_vector_size = M * 2  // Price and volume pairs
    last_som_index = N - 1
    
    for som_index = 0 to last_som_index
        // Initialize state vector with small random values
        initial_state = new_random_vector(state_vector_size, 0.0, -0.1, 0.1)
        // Initialize Q-values with small positive values
        initial_q_values = new_random_vector(num_actions, 0.001, 0.0, 0.002)
        
        som_node = SOMNode.new(initial_state, initial_q_values)
        array.set(som, som_index, som_node)

// Identifies the SOM node with state vector closest to the input vector using the selected distance function.
find_best_node(float[] x) =>
    best_i = 0
    best_node = array.get(som, 0)
    best_d = distance(x, best_node.state)
    last = N - 1
    for i = 1 to last
        node = array.get(som, i)
        d = distance(x, node.state)
        if d < best_d
            best_i := i
            best_node := node
            best_d := d
    [best_i, best_d, best_node]

// Selects the action with highest Q-value from a SOM node, with optional epsilon-greedy exploration.
find_best_q(SOMNode node, p_exploration = 0.0) =>
    if math.random(0.0, 1.0) <= p_exploration
        // Ensure random index is within bounds
        explore_i = math.floor(math.random(0, num_actions - 0.0001))
        explore_i := math.max(0, math.min(explore_i, num_actions - 1))
        [explore_i, node.q_values.get(explore_i)]
    else
        best_action_index = 0
        best_q_value = array.get(node.q_values, 0)
        last_action_index = num_actions - 1
        
        for action_index = 1 to last_action_index
            current_q_value = array.get(node.q_values, action_index)
            if current_q_value > best_q_value
                best_action_index := action_index
                best_q_value := current_q_value
        
        [best_action_index, best_q_value]

// Calculates Gaussian neighborhood influence for SOM learning based on distance from winning node.
gaussian_kernel(int winner_index, int node_index) =>
    distance_squared = math.pow(winner_index - node_index, 2)
    sigma_squared_times_2 = 2.0 * sigma * sigma + numerical_epsilon
    math.exp(-distance_squared / sigma_squared_times_2)

// Updates SOM node states using Gaussian-weighted learning based on distance from the winning node.
update_som_with_neighbors(int winner_index, float[] input_vector) =>
    last_node_index = N - 1
    
    for node_index = 0 to last_node_index
        current_node = array.get(som, node_index)
        neighborhood_influence = gaussian_kernel(winner_index, node_index)
        node_state = current_node.state
        last_state_index = node_state.size() - 1
        
        for state_index = 0 to last_state_index
            input_value = array.get(input_vector, state_index)
            current_state_value = array.get(node_state, state_index)
            updated_value = current_state_value + beta * neighborhood_influence * (input_value - current_state_value)
            array.set(node_state, state_index, updated_value)

// Computes reward signal incorporating returns, volatility penalties, position costs, and directional bonuses.
reward(string action_label, float current_position) =>
    // Base return calculation
    base_return = (source - source_p) / std_price_p
    
    // Volatility penalty for high-risk periods
    calculated_volatility_penalty = math.min(std_price_p * volatility_penalty_factor, volatility_penalty_cap)
    
    // Position sizing penalty (discourage overtrading)
    calculated_position_penalty = math.abs(current_position) * position_penalty_factor
    
    // Directional consistency bonus
    price_direction = math.sign(source - source[1])
    action_direction = str.contains(action_label, "INV") ? 1.0 : 
                      str.contains(action_label, "SELL") ? -1.0 : 0.0
    calculated_directional_bonus = price_direction == action_direction ? directional_bonus : 0.0
    
    // Trading cost
    trading_cost = action_label == "HOLD" ? 0.0 : trading_penalty
    
    // Calculate final reward with tanh normalization
    final_reward = base_return - calculated_volatility_penalty - calculated_position_penalty - trading_cost + calculated_directional_bonus
    tanh(final_reward)

if (bar_index >= learning_start_index) and (bar_index < last_bar_index)
    // First perform Online Learning (per bar)
    // Get current state
    input_now = get_input_vector(M, P)
    input_next = get_input_vector(M)
    [best_i_now, _, best_node_now] = find_best_node(input_now)
    [best_q_i_now, _] = find_best_q(best_node_now, exploration)
    action_label = array.get(action_labels, best_q_i_now)
    // Update SOM
    if 0 == (bar_index % update_som_each_n_bar)
        update_som_with_neighbors(best_i_now, input_now)
    // Get next state
    [_, _, next_node] = find_best_node(input_next)
    [_, next_best_q] = find_best_q(next_node)
    // Update q values
    q_values = best_node_now.q_values
    old_q = array.get(q_values, best_q_i_now)
    r = reward(action_label, strategy.opentrades.capital_held)
    array.set(q_values, best_q_i_now, old_q + beta * (r + gamma * next_best_q - old_q))

    // Then trade when warm up is done
    if (bar_index > learning_start_index + warmup_phase)
        // === Action Selection on Last Bar
        [best_i, best_d, best_node] = find_best_node(input_next)
        [best_q_i, best_q] = find_best_q(best_node)
        action_label = action_labels.get(best_q_i)
        log.info("bar {0}, {1} - i={2}, d={3}, q_i={4}, q={5}, a={6}", bar_index, t_decay, best_i, best_d, best_q_i, best_q, action_label)

        if str.startswith(action_label, "INV")
            qty_str = str.replace(action_label, "INV_", "")
            trade_quantity = str.tonumber(qty_str)
            if not na(trade_quantity) and trade_quantity > 0
                strategy.entry("SOM", strategy.long, qty=trade_quantity)
        else if str.startswith(action_label, "SELL")
            qty_str = str.replace(action_label, "SELL_", "")
            trade_quantity = str.tonumber(qty_str)
            if not na(trade_quantity) and trade_quantity > 0
                strategy.entry("SOM", strategy.short, qty=trade_quantity)
        // HOLD does nothing

plot(math.log(1+strategy.opentrades.capital_held), color=color.new(color.blue, 80), title="Log open trades capital held")
