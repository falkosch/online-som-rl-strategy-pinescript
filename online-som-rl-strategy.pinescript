//@version=6
strategy("Online SOM-RL Strategy", overlay=true)

// === SOM Node definition
type SOMNode
    float[] state   // M-length input vector
    map<string, float> q_values // expected returns by action label

// === Configurable Inputs
source = input.source(close, title="Price source")

M = input.int(50, minval=20, maxval=150, step=10, title="Past price ticks for input vector")
P = input.int(20, minval=10, maxval=100, step=10, title="Future ticks to evaluate reward")
N = input.int(20, minval=10, maxval=50, step=5, title="Number of nodes in Q-table")

initial_exploration = input.float(0.1, minval=0.01, maxval=0.5, step=0.01, title="Initial exploration probability")
initial_sigma_factor = input.float(0.05, minval=0.01, maxval=0.2, step=0.01, title="Initial neighborhood width")
initial_beta = input.float(0.6, minval=0.1, maxval=1.0, step=0.05, title="Learning rate (beta)")
initial_gamma = input.float(0.9, minval=0.5, maxval=0.99, step=0.01, title="Discount factor (gamma)")
trading_penalty = input.float(0.02, minval=0.0, maxval=0.1, step=0.01, title="Trading penalty")

// === Reward function parameters
volatility_penalty_factor = input.float(0.05, minval=0.01, maxval=1.0, step=0.01, title="Volatility penalty factor")
volatility_penalty_cap = input.float(0.1, minval=0.01, maxval=1.0, step=0.01, title="Maximum volatility penalty")
position_penalty_factor = input.float(0.001, minval=0.0001, maxval=0.01, step=0.0001, title="Position sizing penalty factor")
directional_bonus = input.float(0.01, minval=0.0, maxval=0.05, step=0.001, title="Directional consistency bonus")
zscore_clip_range = input.float(3.0, minval=1.0, maxval=5.0, step=0.5, title="Z-score clipping range")

// === Algorithm parameters
distance_function = input.string("cosine", options=["cosine", "euclidean"], title="Distance function for SOM")

t_decay_factor = input.float(0.9992, minval=0.001, maxval=1.0, title="Time decay factor for learning parameters")
update_som_each_n_bar = input.int(2, minval=1, title="How many bars to skip updating the SOM")
delay_phase = input.int(150, minval=0, title="How many bars to wait before the SOM should start to learn")
warmup_phase = input.int(7000, minval=0, title="How many bars to let the SOM learn first")
numerical_epsilon = input.float(1e-6, minval=1e-8, maxval=1e-2, title="To avoid division by zero and improve numerical stability")

// === Source data preparation and validation
safe_source = na(source) ? 0.0 : source

// === Learning algorithm parameters
learning_start_index = math.max(M + P, delay_phase)
t_decay = math.pow(t_decay_factor, bar_index - learning_start_index) + numerical_epsilon
exploration = initial_exploration * t_decay
sigma = N * initial_sigma_factor * t_decay
beta = initial_beta * t_decay
gamma = initial_gamma * t_decay

// === Reward calculation metrics (P-period forward-looking)
source_p = source[P]
std_price_p = ta.stdev(source, P) + numerical_epsilon

// Time-invariant reward calculation using P as forward-looking sample size
// Convert to per-period returns to make time-invariant
per_period_return = math.pow((source / math.max(source_p, numerical_epsilon)), 1.0 / P) - 1.0

// Path quality metrics over P-period window (sample-based, not cumulative)
avg_period_volatility = ta.stdev(ta.roc(source, 1), P) // Per-bar volatility estimate
path_volatility = avg_period_volatility * math.sqrt(P) // Annualized volatility estimate
path_smoothness = 1.0 / (1.0 + path_volatility) // Inverse relationship with volatility

// Maximum drawdown as percentage of highest point in window
max_drawdown_p = (ta.highest(source, P) - ta.lowest(source, P)) / math.max(ta.highest(source, P), numerical_epsilon)
drawdown_penalty = math.max(max_drawdown_p - 0.05, 0.0) // Penalize > 5% max drawdown

// Sharpe-like ratio: per-period return adjusted for path quality
base_return = (per_period_return * path_smoothness - drawdown_penalty) / math.max(avg_period_volatility, numerical_epsilon)

uncapped_volatility_penalty = math.log(math.sqrt(std_price_p)) * volatility_penalty_factor
calculated_volatility_penalty = math.min(uncapped_volatility_penalty, volatility_penalty_cap)
price_direction = math.sign(source - source[1])
risk_adjusted_return = ta.sma(base_return, P)

// === Feature preprocessing statistics (M-period backward-looking)
price_mean_M = ta.sma(safe_source, M)
price_mean_return_M = ta.sma(ta.roc(safe_source, 1), M)
price_stdev_return_M = ta.stdev(ta.roc(safe_source, 1), M)

// === Action options
type TradingAction
    string direction  // "hold", "long", "short"
    float amount     // trade quantity


// Action dictionary mapping labels to TradingAction types
var action_dict = map.new<string, TradingAction>()
if bar_index == 0
    map.put(action_dict, "HOLD", TradingAction.new("hold", 0.0))
    map.put(action_dict, "INV_500", TradingAction.new("long", 500.0))
    map.put(action_dict, "INV_1000", TradingAction.new("long", 1000.0))
    map.put(action_dict, "INV_2000", TradingAction.new("long", 2000.0))
    map.put(action_dict, "INV_5000", TradingAction.new("long", 5000.0))
    map.put(action_dict, "SELL_500", TradingAction.new("short", 500.0))
    map.put(action_dict, "SELL_1000", TradingAction.new("short", 1000.0))
    map.put(action_dict, "SELL_2000", TradingAction.new("short", 2000.0))
    map.put(action_dict, "SELL_5000", TradingAction.new("short", 5000.0))

// Creates a new array of specified size filled with random values within given range.
new_random_vector(int vector_size, float base_value = 0.0, float min_value = 0.0, float max_value = 1.0) =>
    if vector_size <= 0
        runtime.error("Vector size must be positive")
    if min_value > max_value
        runtime.error("Min value cannot be greater than max value")
    
    vector = array.new_float(vector_size)
    last_index = vector_size - 1
    for index = 0 to last_index
        random_value = base_value + math.random(min_value, max_value)
        array.set(vector, index, random_value)
    vector

// Preprocesses price features optimized for cosine distance (pattern-focused, DC-removed).
get_cosine_features(int bars, int bar_offset = 0) =>
    features = array.new_float(bars)
    
    // Use precomputed price mean (adjusted for offset if needed)
    current_price_mean = bar_offset == 0 ? price_mean_M : price_mean_M[bar_offset]
    
    // Process features for cosine distance using direct series access
    for bar_idx = 0 to bars - 1
        // Calculate price return using direct series access
        current_offset = bar_offset + bar_idx
        price_return = 0.0
        if bar_idx > 0
            prev_offset = bar_offset + bar_idx - 1
            price_val = safe_source[current_offset]
            prev_price = safe_source[prev_offset]
            price_return = (price_val - prev_price) / math.max(prev_price, numerical_epsilon)
        
        // Apply DC removal using precomputed mean
        price_feature = price_return - (current_price_mean / math.max(current_price_mean, numerical_epsilon))
        array.set(features, bar_idx, price_feature)
    
    features

// Preprocesses price features optimized for Euclidean distance (z-score normalized).
get_euclidean_features(int bars, int bar_offset = 0) =>
    features = array.new_float(bars)
    
    // Use precomputed statistics (adjusted for offset if needed)
    current_return_mean = bar_offset == 0 ? price_mean_return_M : price_mean_return_M[bar_offset]
    current_return_std = bar_offset == 0 ? price_stdev_return_M : price_stdev_return_M[bar_offset]
    current_return_std_safe = current_return_std + numerical_epsilon
    
    // Apply z-score normalization with clipping using precomputed statistics
    for bar_idx = 0 to bars - 1
        // Calculate price return using direct series access
        price_return = 0.0
        if bar_idx > 0
            current_offset = bar_offset + bar_idx
            prev_offset = bar_offset + bar_idx - 1
            price_val = safe_source[current_offset]
            prev_price = safe_source[prev_offset]
            price_return = (price_val - prev_price) / math.max(prev_price, numerical_epsilon)
        
        // Z-score normalized price returns (clipped to configured range)
        price_zscore = (price_return - current_return_mean) / current_return_std_safe
        price_feature = math.max(-zscore_clip_range, math.min(zscore_clip_range, price_zscore))
        
        array.set(features, bar_idx, price_feature)
    
    features


// Constructs feature vector optimized for the selected distance function with proper normalization and preprocessing.
get_input_vector(int bars, int bar_offset = 0) =>
    if bars <= 0
        runtime.error("Number of bars must be positive")
    
    // Get distance-specific features (price only)
    distance_function == "cosine" ? get_cosine_features(bars, bar_offset) : get_euclidean_features(bars, bar_offset)

// Calculates hyperbolic tangent function for reward normalization.
tanh(x) =>
    exp_x_pos = math.exp(x)
    exp_x_neg = math.exp(-x)
    (exp_x_pos - exp_x_neg) / (exp_x_pos + exp_x_neg + numerical_epsilon)

// Calculates Euclidean distance squared between two vectors for SOM similarity measurement.
squared_distance(float[] a, float[] b) =>
    last = a.size() - 1
    d = 0.0
    for i = 0 to last
        diff = array.get(a, i) - array.get(b, i)
        d += diff * diff
    d

// Calculates cosine distance (1 - cosine similarity) between two vectors for SOM similarity measurement.
cosine_distance(float[] a, float[] b) =>
    last = a.size() - 1
    norm_a = 0.0
    norm_b = 0.0
    dot_ab = 0.0
    for i = 0 to last
        value_a = array.get(a, i)
        value_b = array.get(b, i)
        norm_a += value_a * value_a
        norm_b += value_b * value_b
        dot_ab += value_a * value_b
    1.0 - dot_ab / (math.sqrt(norm_a * norm_b) + numerical_epsilon)

// Selects and applies the configured distance function for vector similarity calculation.
distance(float[] a, float[] b) =>
    switch distance_function
        "cosine" => cosine_distance(a, b)
        "euclidean" => squared_distance(a, b)
        => cosine_distance(a, b)  // default fallback

// === SOM Initialization
var som = array.new<SOMNode>(N)
if bar_index == 0
    // Calculate state size: price returns only
    state_vector_size = M
    last_som_index = N - 1
    
    for som_index = 0 to last_som_index
        // Initialize state vector with small random values
        initial_state = new_random_vector(state_vector_size, 0.0, -0.1, 0.1)
        // Initialize Q-values dictionary with small positive values for each action
        initial_q_values = map.new<string, float>()
        for action_label in map.keys(action_dict)
            map.put(initial_q_values, action_label, math.random(0.0, 0.002) + 0.001)
        
        som_node = SOMNode.new(initial_state, initial_q_values)
        array.set(som, som_index, som_node)

// Identifies the SOM node with state vector closest to the input vector using the selected distance function.
find_best_node(float[] x) =>
    best_i = 0
    best_node = array.get(som, 0)
    best_d = distance(x, best_node.state)
    last = N - 1
    for i = 1 to last
        node = array.get(som, i)
        d = distance(x, node.state)
        if d < best_d
            best_i := i
            best_node := node
            best_d := d
    [best_i, best_d, best_node]

// Selects the action with highest Q-value from a SOM node, with optional epsilon-greedy exploration.
find_best_q(SOMNode node, p_exploration = 0.0) =>
    action_keys = map.keys(node.q_values)
    num_actions = array.size(action_keys)
    
    if math.random(0.0, 1.0) <= p_exploration
        // Generate random action with proper bounds
        explore_i = math.floor(math.random() * num_actions)
        explore_action = array.get(action_keys, explore_i)
        [explore_action, map.get(node.q_values, explore_action)]
    else
        best_action = array.get(action_keys, 0)
        best_q_value = map.get(node.q_values, best_action)
        
        for i = 1 to array.size(action_keys) - 1
            current_action = array.get(action_keys, i)
            current_q_value = map.get(node.q_values, current_action)
            if current_q_value > best_q_value
                best_action := current_action
                best_q_value := current_q_value
        
        [best_action, best_q_value]

// Calculates Gaussian neighborhood influence for SOM learning based on distance from winning node.
gaussian_kernel(int winner_index, int node_index) =>
    distance_squared = math.pow(winner_index - node_index, 2)
    sigma_squared_times_2 = 2.0 * sigma * sigma + numerical_epsilon
    math.exp(-distance_squared / sigma_squared_times_2)

// Updates SOM node states using Gaussian-weighted learning based on distance from the winning node.
update_som_with_neighbors(int winner_index, float[] input_vector) =>
    last_node_index = N - 1
    
    for node_index = 0 to last_node_index
        current_node = array.get(som, node_index)
        neighborhood_influence = gaussian_kernel(winner_index, node_index)
        node_state = current_node.state
        last_state_index = node_state.size() - 1
        
        for state_index = 0 to last_state_index
            input_value = array.get(input_vector, state_index)
            current_state_value = array.get(node_state, state_index)
            updated_value = current_state_value + beta * neighborhood_influence * (input_value - current_state_value)
            array.set(node_state, state_index, updated_value)

// Computes reward signal incorporating returns, volatility penalties, position costs, and directional bonuses.
reward(string action_label, float current_position) =>
    // Position sizing penalty (discourage overtrading)
    calculated_position_penalty = math.abs(current_position) * position_penalty_factor
    
    // Directional consistency bonus
    action = map.get(action_dict, action_label)
    action_direction = not na(action) ? (action.direction == "long" ? 1.0 : action.direction == "short" ? -1.0 : 0.0) : 0.0
    calculated_directional_bonus = price_direction == action_direction ? directional_bonus : 0.0
    
    // Trading cost
    trading_cost = (not na(action) and action.direction != "hold") ? trading_penalty : 0.0
    
    // Calculate final reward with tanh normalization using precomputed series
    final_reward = base_return - calculated_volatility_penalty - calculated_position_penalty - trading_cost + calculated_directional_bonus
    tanh(final_reward)


current_reward = 0.0

if (bar_index >= learning_start_index) and (bar_index < last_bar_index)
    // First perform Online Learning (per bar)
    // Get current state
    input_now = get_input_vector(M, P)
    input_next = get_input_vector(M, 0)
    [best_i_now, _, best_node_now] = find_best_node(input_now)
    [action_label, _] = find_best_q(best_node_now, exploration)
    // Update SOM
    if 0 == (bar_index % update_som_each_n_bar)
        update_som_with_neighbors(best_i_now, input_now)
    // Get next state
    [_, _, next_node] = find_best_node(input_next)
    [_, next_best_q] = find_best_q(next_node)
    // Update q values
    q_values = best_node_now.q_values
    old_q = map.get(q_values, action_label)
    current_reward := reward(action_label, strategy.opentrades.capital_held)
    map.put(q_values, action_label, old_q + beta * (current_reward + gamma * next_best_q - old_q))

    // Then trade when warm up is done
    if (bar_index > learning_start_index + warmup_phase)
        // === Action Selection on Last Bar
        [best_i, best_d, best_node] = find_best_node(input_next)
        [action_label, best_q] = find_best_q(best_node)
        
        
        log.info("bar {0}, {1} - i={2}, d={3}, a={4}, q={5}", bar_index, t_decay, best_i, best_d, action_label, best_q)

        action = map.get(action_dict, action_label)
        if not na(action)
            switch action.direction
                "long" => 
                    if action.amount > 0
                        strategy.entry("SOM", strategy.long, qty=action.amount)
                "short" => 
                    if action.amount > 0
                        strategy.entry("SOM", strategy.short, qty=action.amount)
                // "hold" does nothing

// Debug plots
plot(current_reward, color=color.new(color.red, 0), title="Reward Function Value")
